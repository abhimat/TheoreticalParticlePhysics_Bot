{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.backend import manual_variable_initialization\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "print('TensorFlow version: {0}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_location = '/home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Seeding\n",
    "\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = save_files_location + 'TEP_talks.txt'\n",
    "with open(filepath) as f:\n",
    "    TEP_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bootstrapping Inflationary Fluctuations\n",
      "Abstract: In flat space, four point scattering amplitudes at weak coupling can be fully determined from Lorentz symmetry, unitarity and causality. The resulting scattering amplitude depends on model details only through coupling constants and the particle content of the theory. I will show how the analogous story works in the case of inflationary fluctuations. We found explicit expressions for inflationary three and four-point functions, whose shape\n"
     ]
    }
   ],
   "source": [
    "print(TEP_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"$%&\\'()+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_abcdefghijklmnopqrstuvwxyz{}~×éöˆ̈ℓ∗∼≤'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted(set(TEP_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "tokenizer.fit_on_texts(TEP_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 20, 8, 3, 9, 5, 12, 3, 31]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Abstract:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A b s t r a c t :']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[32, 20, 8, 3, 9, 5, 12, 3, 31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id = 100\n",
      "dataset_size = 136477\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # Number of distinct characters\n",
    "dataset_size = tokenizer.document_count # Total number of characters\n",
    "\n",
    "print('max_id = {0}'.format(max_id))\n",
    "print('dataset_size = {0}'.format(dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([TEP_text])) - 1\n",
    "train_size = dataset_size * 100 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chop dataset into windows (for Stateful RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop dataset into windows\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten windows\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch, and no shuffling\n",
    "\n",
    "# dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "# dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "# dataset = dataset.map(\n",
    "#     lambda X_batch, Y_batch: (embedding(X_batch), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim = max_id + num_oov_buckets,\n",
    "                           batch_input_shape=[batch_size, None],\n",
    "                           output_dim = 2),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, 2]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2\n",
    "                    ),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation='softmax'))\n",
    "])\n",
    "\n",
    "# Stateful RNNs: dropouts don't work (https://github.com/ageron/handson-ml2/issues/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load_file = save_files_location + '/TEP_Bot_Save_2020-05-07_stateful.h5'\n",
    "model_load = keras.models.load_model(model_load_file)\n",
    "\n",
    "model.set_weights(model_load.get_weights())\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs):\n",
    "        self.model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (16, None, 2)             200       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (16, None, 128)           50688     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (16, None, 128)           99072     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 100)         12900     \n",
      "=================================================================\n",
      "Total params: 162,860\n",
      "Trainable params: 162,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model_stateless = keras.models.load_model('TEP_Bot_Save_2020-05-05.h5')\n",
    "# print('Weights:')\n",
    "# print(model_stateless.get_weights())\n",
    "\n",
    "# model = tf.keras.models.load_model('TEP_Bot_Save_2020-05-03.h5')\n",
    "\n",
    "weights_file_load = save_files_location + 'TEP_Bot_2020-05-07_weights.pkl'\n",
    "\n",
    "# with open(weights_file_load, 'rb') as in_pickle:\n",
    "#     weights = pickle.load(in_pickle)\n",
    "\n",
    "# model.set_weights(weights)\n",
    "\n",
    "# model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "save_model_file = save_files_location + 'TEP_Bot_Save_2020-05-07_stateful.h5'\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=save_model_file,\n",
    "                                                 verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 85 steps\n",
      "Epoch 1/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2818\n",
      "Epoch 00001: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 187s 2s/step - loss: 1.2820\n",
      "Epoch 2/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2656\n",
      "Epoch 00002: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 209s 2s/step - loss: 1.2658\n",
      "Epoch 3/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2514\n",
      "Epoch 00003: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 236s 3s/step - loss: 1.2516\n",
      "Epoch 4/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2382\n",
      "Epoch 00004: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 191s 2s/step - loss: 1.2384\n",
      "Epoch 5/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2258\n",
      "Epoch 00005: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 188s 2s/step - loss: 1.2259\n",
      "Epoch 6/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2138\n",
      "Epoch 00006: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 192s 2s/step - loss: 1.2140\n",
      "Epoch 7/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.2024\n",
      "Epoch 00007: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 248s 3s/step - loss: 1.2025\n",
      "Epoch 8/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1914\n",
      "Epoch 00008: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 194s 2s/step - loss: 1.1915\n",
      "Epoch 9/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1810\n",
      "Epoch 00009: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 172s 2s/step - loss: 1.1810\n",
      "Epoch 10/50\n",
      "84/85 [============================>.] - ETA: 1s - loss: 1.1709\n",
      "Epoch 00010: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 165s 2s/step - loss: 1.1710\n",
      "Epoch 11/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1617\n",
      "Epoch 00011: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 175s 2s/step - loss: 1.1618\n",
      "Epoch 12/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1536\n",
      "Epoch 00012: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 185s 2s/step - loss: 1.1537\n",
      "Epoch 13/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1469\n",
      "Epoch 00013: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 189s 2s/step - loss: 1.1470\n",
      "Epoch 14/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1427\n",
      "Epoch 00014: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 216s 3s/step - loss: 1.1428\n",
      "Epoch 15/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1438\n",
      "Epoch 00015: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 240s 3s/step - loss: 1.1444\n",
      "Epoch 16/50\n",
      "84/85 [============================>.] - ETA: 1s - loss: 1.1411\n",
      "Epoch 00016: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 165s 2s/step - loss: 1.1414\n",
      "Epoch 17/50\n",
      "84/85 [============================>.] - ETA: 2s - loss: 1.1326\n",
      "Epoch 00017: saving model to /home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/TEP_Bot_Save_2020-05-07_stateful.h5\n",
      "85/85 [==============================] - 171s 2s/step - loss: 1.1328\n",
      "Epoch 18/50\n",
      "22/85 [======>.......................] - ETA: 2:00 - loss: 1.1019"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model.fit(dataset,\n",
    "                    steps_per_epoch = train_size // batch_size // n_steps,\n",
    "                    epochs=50,\n",
    "                    callbacks = [cp_callback, ResetStatesCallback()]\n",
    "                   )\n",
    "\n",
    "model.save(save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "\n",
    "weights_file_save = save_files_location + 'TEP_Bot_2020-05-07_stateful_weights.pkl'\n",
    "\n",
    "with open(weights_file_save, 'wb') as out_pickle:\n",
    "    pickle.dump(weights, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load('TEP_Bot_Save_2020-05-02.h5')\n",
    "# with open('history_2020-05-03.pkl', 'wb') as in_pickle:\n",
    "#     history = pickle.load(in_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['acc'])\n",
    "plt.plot(history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ready to go')\n",
    "\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return embedding(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = preprocess([\"In this tal\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
