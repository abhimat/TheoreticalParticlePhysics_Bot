{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import sklearn\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.backend import manual_variable_initialization\n",
    "# manual_variable_initialization(True)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "print('TensorFlow version: {0}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_files_location = '/home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Seeding\n",
    "\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"nlp\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = save_files_location + 'TEP_talks.txt'\n",
    "with open(filepath) as f:\n",
    "    TEP_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Bootstrapping Inflationary Fluctuations\n",
      "Abstract: In flat space, four point scattering amplitudes at weak coupling can be fully determined from Lorentz symmetry, unitarity and causality. The resulting scattering amplitude depends on model details only through coupling constants and the particle content of the theory. I will show how the analogous story works in the case of inflationary fluctuations. We found explicit expressions for inflationary three and four-point functions, whose shape\n"
     ]
    }
   ],
   "source": [
    "print(TEP_text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"$%&\\'()+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]^_abcdefghijklmnopqrstuvwxyz{}~×éöˆ̈ℓ∗∼≤'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(sorted(set(TEP_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True, lower=False)\n",
    "tokenizer.fit_on_texts(TEP_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 20, 8, 3, 9, 5, 12, 3, 31]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['Abstract:'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A b s t r a c t :']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[32, 20, 8, 3, 9, 5, 12, 3, 31]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_id = 100\n",
      "dataset_size = 136477\n"
     ]
    }
   ],
   "source": [
    "max_id = len(tokenizer.word_index) # Number of distinct characters\n",
    "dataset_size = tokenizer.document_count # Total number of characters\n",
    "\n",
    "print('max_id = {0}'.format(max_id))\n",
    "print('dataset_size = {0}'.format(dataset_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([TEP_text])) - 1\n",
    "train_size = dataset_size * 95 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "\n",
    "dataset_validation = tf.data.Dataset.from_tensor_slices(encoded[train_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chop dataset into windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chop dataset into windows\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)\n",
    "dataset_validation = dataset_validation.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten windows\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset_validation = dataset_validation.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch, shuffle\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "dataset_validation = dataset_validation.shuffle(10000).batch(batch_size)\n",
    "dataset_validation = dataset_validation.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "\n",
    "dataset = dataset.prefetch(1)\n",
    "dataset_validation = dataset_validation.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 0\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim = max_id + num_oov_buckets,\n",
    "                           output_dim = 2),\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, 2],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model_file = save_files_location + 'TEP_Bot_Save_2020-05-07.h5'\n",
    "model = keras.models.load_model(load_model_file)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 2)           200       \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, None, 128)         50688     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, None, 128)         99072     \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 100)         12900     \n",
      "=================================================================\n",
      "Total params: 162,860\n",
      "Trainable params: 162,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "manual_variable_initialization(True)\n",
    "\n",
    "weights_file_load = save_files_location + 'TEP_Bot_2020-05-07_weights.pkl'\n",
    "\n",
    "# with open(weights_file_load, 'rb') as in_pickle:\n",
    "#     weights = pickle.load(in_pickle)\n",
    "\n",
    "# model.set_weights(weights)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "save_model_file = save_files_location + 'TEP_Bot_Save_2020-05-07.h5'\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = keras.callbacks.ModelCheckpoint(filepath=save_model_file,\n",
    "                                              verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 4051 steps, validate for 213 steps\n",
      "Epoch 1/10\n",
      " 975/4051 [======>.......................] - ETA: 3:00:40 - loss: 1.9441"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset,\n",
    "                    validation_data=dataset_validation,\n",
    "                    validation_steps=train_size // batch_size // 19,\n",
    "                    steps_per_epoch=train_size // batch_size,\n",
    "                    epochs=10,\n",
    "                    initial_epoch=0,\n",
    "                    callbacks = [cp_callback]\n",
    "                   )\n",
    "\n",
    "model.save(save_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.get_weights()\n",
    "\n",
    "weights_file_save = save_files_location + 'TEP_Bot_2020-05-07_weights.pkl'\n",
    "\n",
    "with open(weights_file_save, 'wb') as out_pickle:\n",
    "    pickle.dump(weights, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/idies/workspace/Storage/abhimat/persistent/TEP_Bot/history_2020-05-04.pkl', 'wb') as out_pickle:\n",
    "#     pickle.dump(history, out_pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (py37)",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
